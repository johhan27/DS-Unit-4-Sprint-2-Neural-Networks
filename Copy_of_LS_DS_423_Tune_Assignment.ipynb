{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of LS_DS_423_Tune_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johhan27/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/Copy_of_LS_DS_423_Tune_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "\n",
        "\n",
        "# Tune Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
        "\n",
        "# Gridsearch Hyperparameters\n",
        "\n",
        "In the guided project, you learned how to use sklearn's GridsearchCV and keras-tuner library to tune the hyperparameters of a neural network model. For your module project, you'll continue using these two libraries; however, we will make things a little more interesting for you. \n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n",
        "\n",
        "\n",
        "\n",
        "**Don't forget to switch to GPU on Colab!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lzoonloI0Ot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de0204ad-ccfa-4fa0-f21c-92ab83a3f06e"
      },
      "source": [
        "# native python libraries imports \n",
        "import math\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# sklearn imports \n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import tensorflow as tf #\n",
        "from tensorflow import keras #\n",
        "# keras imports \n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "!pip install -q -U keras-tuner #\n",
        "from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "from keras.activations import relu, sigmoid\n",
        "from tensorflow.keras.optimizers import Adam, SGD #\n",
        "from tensorflow.keras.utils import get_file #\n",
        "\n",
        "# required for compatibility between sklearn and keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 31.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 33.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 34.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 36.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 7.1 MB/s \n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxXxCMJpI0Ou"
      },
      "source": [
        "def load_quickdraw10():\n",
        "    \"\"\"\n",
        "    Fill out this docstring, and comment the code for practice in writing the kind of code that will get you hired. \n",
        "    \"\"\"\n",
        "    \n",
        "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
        "    \n",
        "    path_to_zip = get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
        "\n",
        "    data = np.load(path_to_zip)\n",
        "    \n",
        "    # normalize your image data\n",
        "    max_pixel_value = 255\n",
        "    X = data['arr_0']/max_pixel_value\n",
        "    Y = data['arr_1']\n",
        "        \n",
        "    return train_test_split(X, Y, shuffle=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45m5XaCFI0Ou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2024c6f4-889a-47fc-a96a-ff915ba5e684"
      },
      "source": [
        "X_train, X_test, y_train, y_test = load_quickdraw10()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\n",
            "25427968/25421363 [==============================] - 0s 0us/step\n",
            "25436160/25421363 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf11zxcCI0Ov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78636591-1f75-4a8e-fea5-4ed2238e151a"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CoZCMoII0Ov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd1d03b-5eab-4947-9c92-9704e035fdb0"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pugKBJQ2I0Ow"
      },
      "source": [
        "_____\n",
        "\n",
        "# Experiment 1\n",
        "\n",
        "## Tune Hyperparameters Using Enhanced GridsearchCV \n",
        "\n",
        "We will use GridsearchCV again to tune a deep learning model; however, we will add some additional functionality to our gridsearch. Specifically, we will automate away the generation of how many nodes to use in a layer and how many layers to use in a model!\n",
        "\n",
        "By the way, yes, there is a function within a function. Try not to let that bother you. An alternative to this would be to create a class. If you're up for the challenge, give it a shot. However, consider this a stretch goal that you come back to after going through this assignment. \n",
        "\n",
        "\n",
        "### Objective \n",
        "\n",
        "This experiment aims to show you how to automate the generation of layers and layer nodes for gridsearch. Up until now, we've been manually selecting the number of layers and layer nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
        "    \"\"\"\"\n",
        "    Returns a complied keras model \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_layers: int \n",
        "        number of hidden layers in the model \n",
        "        To be clear, this excludes the input and output layers.\n",
        "        \n",
        "    first_layer_nodes: int\n",
        "        Number of nodes in the first hidden layer \n",
        "\n",
        "    last_layer_nodes: int\n",
        "        Number of nodes in the last hidden layer (this is the layer before the output layer)\n",
        "        \n",
        "     act_funct: string \n",
        "         Name of activation function to use in hidden layers (this excludes the output layer)\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object \n",
        "    \"\"\"\n",
        "    \n",
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer. \n",
        "        To be clear, this excludes the input and output layers. \n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        The number of nodes in each layer is linearly incremented. \n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            The number of hidden layers\n",
        "            These values should be 2 or greater \n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains the number of nodes for each layer \n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2 \n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
        "        # when set to True number of nodes, are decreased for subsequent layers \n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from the previous layer's nodes to increment towards smaller numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            \n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
        "            nodes_increment = (first_layer_nodes - last_layer_nodes)/ (n_layers-1)\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(math.ceil(nodes))\n",
        "\n",
        "            # increment nodes for next layer \n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return layers\n",
        "    \n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "    \n",
        "    for i in range(1, n_layers):\n",
        "        if i==1:\n",
        "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
        "        else:\n",
        "            model.add(Dense(n_nodes[i-1], activation=act_funct))\n",
        "            \n",
        "            \n",
        "    # output layer \n",
        "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
        "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', \n",
        "                  optimizer='adam', # adam is a good default optimizer \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # do not include model.fit() inside the create_model function\n",
        "    # KerasClassifier is expecting a compiled model \n",
        "    return model\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLJcMMvDI0Oz"
      },
      "source": [
        "## Explore Create_Model\n",
        "\n",
        "Let's build a few different models to understand how the above code works in practice. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2MXQbEjI0Oz"
      },
      "source": [
        "### Build Model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "gHH9RRINI0O0"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "model = create_model(n_layers=10,  first_layer_nodes=500, last_layer_nodes=100, act_funct =\"relu\", negative_node_incrementation=True)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwUyuxOiI0O0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34ea51f-9fb0-4b14-c253-e2aa9da5cb9d"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes has been linearly incremented in decreasing values. \n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 500)               392500    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 456)               228456    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 412)               188284    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 367)               151571    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 323)               118864    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 278)               90072     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 234)               65286     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 189)               44415     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 145)               27550     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                1460      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,308,458\n",
            "Trainable params: 1,308,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhqcE5zEI0O0"
      },
      "source": [
        "### Build Model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = False`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e0722533c325d699f4842e874e43720e",
          "grade": false,
          "grade_id": "cell-99d563a291231a7b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "xtjvYSjCI0O1"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "model = create_model(n_layers=10,  first_layer_nodes=500, last_layer_nodes=100, act_funct =\"relu\", negative_node_incrementation=False)\n",
        "\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-A5IJJUI0O1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74b40593-7543-4efa-9ce4-b26c9a9ed3bf"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes has been linearly incremented in increasing values.\n",
        "# The output layer must have 10 nodes because there are 10 labels to predict \n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_10 (Dense)            (None, 500)               392500    \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 545)               273045    \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 589)               321594    \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 634)               374060    \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 678)               430530    \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 723)               490917    \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 767)               555308    \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 812)               623616    \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 856)               695928    \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                8570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,166,068\n",
            "Trainable params: 4,166,068\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXCjnun6I0O1"
      },
      "source": [
        "# feel free to play around with parameters to gain additional insight as to how the create_model function works \n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxaShY_-I0O2"
      },
      "source": [
        "Ok, now that we've played around a bit with  `create_model` to understand how it works, let's build a much simpler model that we'll be running gridsearches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9oHYqblI0O2"
      },
      "source": [
        "### Build Model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 2` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "606b85d0ba4531836f97caf6850297f8",
          "grade": false,
          "grade_id": "cell-4ca6c5e51302fd10",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5UpVuVk2I0O2"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "model = create_model(n_layers=2,  first_layer_nodes=500, last_layer_nodes=100, act_funct =\"relu\", negative_node_incrementation=True)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjNMxvG4I0O2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e39da2a-5a94-4028-d8cb-881ad4754aa8"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_20 (Dense)            (None, 500)               392500    \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 10)                5010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 397,510\n",
            "Trainable params: 397,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1XFAIRsI0O2"
      },
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'n_layers': [2, 3],\n",
        "              'epochs': [3], \n",
        "              \"first_layer_nodes\": [500, 300],\n",
        "              \"last_layer_nodes\": [100, 50]\n",
        "             }"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EJCj9myI0O3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77377f47-3c78-4451-a296-3b44bcf66d47"
      },
      "source": [
        "model = KerasClassifier(create_model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGJhDcxFI0O3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547414c2-434d-40b8-8be3-2633f62986a2"
      },
      "source": [
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6650 - accuracy: 0.8017\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4502 - accuracy: 0.8658\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3632 - accuracy: 0.8917\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4682 - accuracy: 0.8651\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6590 - accuracy: 0.8045\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4434 - accuracy: 0.8685\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3558 - accuracy: 0.8934\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4803 - accuracy: 0.8619\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6671 - accuracy: 0.7999\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4556 - accuracy: 0.8630\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3662 - accuracy: 0.8905\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4588 - accuracy: 0.8657\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6352 - accuracy: 0.8051\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4320 - accuracy: 0.8679\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3487 - accuracy: 0.8931\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4711 - accuracy: 0.8633\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.6356 - accuracy: 0.8049\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4264 - accuracy: 0.8704\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3381 - accuracy: 0.8954\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4612 - accuracy: 0.8662\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.6380 - accuracy: 0.8060\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4338 - accuracy: 0.8681\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3487 - accuracy: 0.8929\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4342 - accuracy: 0.8734\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6632 - accuracy: 0.8023\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4492 - accuracy: 0.8690\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3604 - accuracy: 0.8918\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4612 - accuracy: 0.8665\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6609 - accuracy: 0.8040\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4463 - accuracy: 0.8667\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3582 - accuracy: 0.8931\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4800 - accuracy: 0.8591\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6735 - accuracy: 0.7968\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4571 - accuracy: 0.8642\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3663 - accuracy: 0.8914\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4527 - accuracy: 0.8676\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6389 - accuracy: 0.8041\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4390 - accuracy: 0.8664\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3503 - accuracy: 0.8935\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4659 - accuracy: 0.8636\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.6284 - accuracy: 0.8067\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4274 - accuracy: 0.8701\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3389 - accuracy: 0.8957\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4671 - accuracy: 0.8653\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6409 - accuracy: 0.8029\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4373 - accuracy: 0.8671\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3515 - accuracy: 0.8936\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4402 - accuracy: 0.8709\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6896 - accuracy: 0.7929\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4753 - accuracy: 0.8585\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3926 - accuracy: 0.8835\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4824 - accuracy: 0.8601\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6837 - accuracy: 0.7939\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4661 - accuracy: 0.8623\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3831 - accuracy: 0.8848\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4961 - accuracy: 0.8538\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6979 - accuracy: 0.7926\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4819 - accuracy: 0.8561\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3931 - accuracy: 0.8822\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4658 - accuracy: 0.8632\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6571 - accuracy: 0.8006\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4516 - accuracy: 0.8636\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3666 - accuracy: 0.8874\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4697 - accuracy: 0.8602\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6452 - accuracy: 0.8048\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4408 - accuracy: 0.8671\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3584 - accuracy: 0.8895\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4851 - accuracy: 0.8586\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6595 - accuracy: 0.8003\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4531 - accuracy: 0.8631\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3690 - accuracy: 0.8868\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4574 - accuracy: 0.8651\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6858 - accuracy: 0.7966\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4738 - accuracy: 0.8584\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3903 - accuracy: 0.8851\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4818 - accuracy: 0.8608\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6825 - accuracy: 0.7965\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4644 - accuracy: 0.8633\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3809 - accuracy: 0.8863\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4845 - accuracy: 0.8599\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6913 - accuracy: 0.7930\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4762 - accuracy: 0.8600\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3939 - accuracy: 0.8822\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4735 - accuracy: 0.8604\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6572 - accuracy: 0.7997\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4484 - accuracy: 0.8646\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3642 - accuracy: 0.8885\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4650 - accuracy: 0.8601\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6453 - accuracy: 0.8036\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4406 - accuracy: 0.8664\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3607 - accuracy: 0.8890\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4848 - accuracy: 0.8584\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6641 - accuracy: 0.7966\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4543 - accuracy: 0.8625\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3707 - accuracy: 0.8863\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4813 - accuracy: 0.8575\n",
            "Epoch 1/3\n",
            "2344/2344 [==============================] - 14s 6ms/step - loss: 0.5895 - accuracy: 0.8213\n",
            "Epoch 2/3\n",
            "2344/2344 [==============================] - 14s 6ms/step - loss: 0.4086 - accuracy: 0.8756\n",
            "Epoch 3/3\n",
            "2344/2344 [==============================] - 14s 6ms/step - loss: 0.3329 - accuracy: 0.8973\n",
            "Best: 0.8676266471544901 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8642133275667826, Stdev: 0.0016679911742290506 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8676266471544901, Stdev: 0.004221127878931537 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8643866777420044, Stdev: 0.003780132850093694 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8666266798973083, Stdev: 0.003112349364329407 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8590400020281473, Stdev: 0.0039127860256402075 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8613066673278809, Stdev: 0.0027744149440966878 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8603600263595581, Stdev: 0.00035925971033623656 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8586533268292745, Stdev: 0.0010655073035890177 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urhe6JWmI0O4"
      },
      "source": [
        "best_model = grid_result.best_estimator_"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO0TkEJ0I0O4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67960a6c-e0d5-4dbf-b033-3bc574d958b0"
      },
      "source": [
        "best_model.get_params()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'build_fn': <function __main__.create_model>,\n",
              " 'epochs': 3,\n",
              " 'first_layer_nodes': 500,\n",
              " 'last_layer_nodes': 100,\n",
              " 'n_layers': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cydDGYdYI0O4"
      },
      "source": [
        "-----\n",
        "\n",
        "# Experiment 2\n",
        "\n",
        "## Benchmark Different Optimization Algorithms \n",
        "\n",
        "In this section, we are going to use the same model and dataset to benchmark 3 different gridsearch approaches: \n",
        "\n",
        "- Random Search\n",
        "- Bayesian Optimization\n",
        "- Brute Force Gridsearch\n",
        "\n",
        "Our goal in this experiment is two-fold. We want to see which approach: \n",
        "\n",
        "- Scores the highest accuracy\n",
        "- Has the shortest run time \n",
        "\n",
        "We want to see how these 3 gridsearch approaches handle these trade-offs and give you a sense of those trade-offs.\n",
        "\n",
        "### Trade-Offs\n",
        "\n",
        "`Brute Force Gridsearch` will train a model on every unique hyperparameter combination; this guarantees that you'll get the highest possible accuracy from your parameter set, but your gridsearch might have a very long run-time.\n",
        "\n",
        "`Random Search` will randomly sample from your parameter set, which, depending on how many samples, the run-time might be significantly cut down. Still, you might or might not sample the parameters that correspond to the highest possible accuracies.\n",
        "\n",
        "`Bayesian Optimization` has a bit of intelligence built into its search algorithm, but you must manually select some parameters that greatly influence the model learning outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oreSpsCWI0O4"
      },
      "source": [
        "-------\n",
        "### Build our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drNaKnp8I0O4"
      },
      "source": [
        "# because gridsearching can take a lot of time, and we are bench-marking 3 different approaches\n",
        "# let's build a simple model to minimize run time \n",
        "\n",
        "def build_model(hp):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a compiled keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "  "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLpWzs86I0O5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "85072ff5-6990-4d42-86a9-8d3e0a7e521c"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hp = HyperParameters()\n",
        "hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
        "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'relu'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJN2yATmI0O5"
      },
      "source": [
        "------\n",
        "# Run the Gridsearch Algorithms \n",
        "\n",
        "### Random Search\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aaff9aae33845f374e15f2381719d83a",
          "grade": false,
          "grade_id": "cell-8c1dfb9b6d12bea2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "I6fWPOeyI0O5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770dc8df-5503-43c5-944e-2a22ff665229"
      },
      "source": [
        "# how many unique hyperparameter combinations do we have? \n",
        "# HINT: take the product of the number of possible values for each hyperparameter \n",
        "# save your answer to n_unique_hparam_combos\n",
        "\n",
        "n_unique_hparam_combos = 16*3*2\n",
        "print(n_unique_hparam_combos)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9d628451e83431e1b52da10eccf2c00",
          "grade": false,
          "grade_id": "cell-1fa83950bb2d5f92",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "-c7AUuMkI0O5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa04a557-c91e-4124-fd21-e0d3281c5528"
      },
      "source": [
        "# how many of these do we want to randomly sample?\n",
        "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
        "# save this number to n_param_combos_to_sample\n",
        "\n",
        "n_param_combos_to_sample = math.ceil(n_unique_hparam_combos*0.25)\n",
        "print(n_param_combos_to_sample)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB05gTlII0O5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e607c97-b5b7-432d-e122-2bbb35360408"
      },
      "source": [
        "%time\n",
        "random_tuner = RandomSearch(\n",
        "            build_model,\n",
        "            objective='val_accuracy',\n",
        "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
        "            seed=1234,\n",
        "            hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "            directory='./keras-tuner-trial',\n",
        "            project_name='random_search')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
            "Wall time: 9.06 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsQ0Cf2KI0O5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36fdf9fc-464d-442b-a9e3-8e8d8372d37b"
      },
      "source": [
        "# take note of Total elapsed time in print out\n",
        "%%time\n",
        "\n",
        "\"\"\"\n",
        "random_tuner.search(X_train, y_train,\n",
        "                    epochs=3,\n",
        "                    validation_data=(X_test, y_test))\n",
        "\"\"\"\n",
        "\n",
        "no need to run it everytime"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 25 Complete [00h 00m 41s]\n",
            "val_accuracy: 0.7108399868011475\n",
            "\n",
            "Best val_accuracy So Far: 0.8722800016403198\n",
            "Total elapsed time: 00h 14m 01s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "CPU times: user 12min 48s, sys: 47 s, total: 13min 35s\n",
            "Wall time: 13min 20s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f80fCuyfI0O5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dab5acf-464d-424f-df2f-afbccf0cf9dc"
      },
      "source": [
        "# identify the best score and hyperparameter (should be at the top since scores are ranked)\n",
        "\n",
        "\"\"\"\n",
        "random_tuner.results_summary()\n",
        "\"\"\"\n",
        "\n",
        "#uncomment if necessary to run above again"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/random_search\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8722800016403198\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8702800273895264\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.864799976348877\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 448\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8624799847602844\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 416\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8581200242042542\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 288\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8559600114822388\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8511199951171875\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 320\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8332800269126892\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8318799734115601\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 160\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8318399786949158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "2-22HMaYI0O6"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the best performing hyperparameter combination and model score. Note that because this is a random search, multiple runs might have slightly different outcomes.\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f084b5d373f8589a1de8d6d4473b974a",
          "grade": true,
          "grade_id": "cell-5527738b6382c164",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "uVtC1cIQI0O6"
      },
      "source": [
        "Trial summary\n",
        "Hyperparameters:\n",
        "units: 352\n",
        "learning_rate: 0.001\n",
        "activation: relu\n",
        "Score: 0.8722800016403198"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5LxscSjI0O6"
      },
      "source": [
        "------\n",
        "### Bayesian Optimization\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
        "\n",
        "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`. \n",
        "\n",
        "`num_initial_points`: \n",
        "\n",
        "Number of randomly selected hyperparameter combinations to try before applying Bayesian probability to determine the likelihood of which param combo to try next based on expected improvement\n",
        "\n",
        "\n",
        "`beta`: \n",
        "\n",
        "Larger values mean more willingness to explore new hyperparameter combinations (analogous to searching for the global minimum in gradient descent). Conversely, smaller values mean less willingness to try new hyperparameter combinations (analogous to getting stuck in a local minimum in gradient descent). \n",
        "\n",
        "As a start, err on the side of larger values. What defines a small or large value, you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_lv5tqyI0O6"
      },
      "source": [
        "# we know that 24 samples is about 25% of 96 possible hyper-parameter combos\n",
        "# because BO isn't random (after num_initial_points number of trails) let's see if 15 max trials gives good results\n",
        "# feel free to play with any of these numbers\n",
        "max_trials=15\n",
        "num_initial_points=5\n",
        "beta=5.0"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xCtk3nVI0O6"
      },
      "source": [
        "bayesian_tuner = BayesianOptimization(\n",
        "                    build_model,\n",
        "                    objective='val_accuracy',\n",
        "                    max_trials=max_trials,\n",
        "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "                    num_initial_points=num_initial_points, \n",
        "                    beta=beta, \n",
        "                    seed=1234,\n",
        "                    directory='./keras-tuner-trial',\n",
        "                    project_name='bayesian_optimization_4')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Myq4RDSgI0O6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d136dac-b2b1-4602-fb36-21b33409f46e"
      },
      "source": [
        "%%time\n",
        "bayesian_tuner.search(X_train, y_train,\n",
        "               epochs=3,\n",
        "               validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9 Complete [00h 00m 21s]\n",
            "val_accuracy: 0.8290799856185913\n",
            "\n",
            "Best val_accuracy So Far: 0.8791199922561646\n",
            "Total elapsed time: 00h 04m 24s\n",
            "\n",
            "Search: Running Trial #10\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "units             |512               |512               \n",
            "learning_rate     |0.1               |0.001             \n",
            "activation        |sigmoid           |relu              \n",
            "\n",
            "Epoch 1/3\n",
            "2344/2344 [==============================] - 13s 5ms/step - loss: 1.7161 - accuracy: 0.5911 - val_loss: 1.7672 - val_accuracy: 0.6194\n",
            "Epoch 2/3\n",
            " 431/2344 [====>.........................] - ETA: 7s - loss: 1.6098 - accuracy: 0.6252"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "4xQX_EUKI0O6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "135ced3c-63c5-46f4-b653-99ffca5ca789"
      },
      "source": [
        "bayesian_tuner.results_summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8791199922561646\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8723199963569641\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8659200072288513\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8553199768066406\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8290799856185913\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.01\n",
            "activation: relu\n",
            "Score: 0.8265200257301331\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8252800107002258\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8241999745368958\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8239200115203857\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.01\n",
            "activation: relu\n",
            "Score: 0.8231599926948547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R33cbZ26I0O7"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the best performing hyperparameter combination and model score. Note that because this is Bayesian Optimization, multiple runs might have slightly different outcomes.\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
          "grade": true,
          "grade_id": "cell-ff95600bf745f40f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Z7cbqTgNI0O7"
      },
      "source": [
        "Trial summary\n",
        "Hyperparameters:\n",
        "units: 512\n",
        "learning_rate: 0.001\n",
        "activation: relu\n",
        "Score: 0.8791199922561646"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMwSVnUJI0O7"
      },
      "source": [
        "---------\n",
        "## Brute Force Gridsearch Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0W4bCd_I0O7"
      },
      "source": [
        "### Populate a Sklearn Compatible Parameter Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT_ytR-uI0O7"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hyper_parameters = {\n",
        "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
        "    \"units\": np.arange(32, 544, 32).tolist(),\n",
        "    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
        "    \"activation\":[\"relu\", \"sigmoid\"]\n",
        "}"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC6we-hhI0O7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8609ac01-c73d-4af3-c7fc-64d7d4f5cf62"
      },
      "source": [
        "hyper_parameters"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': ['relu', 'sigmoid'],\n",
              " 'learning_rate': [0.1, 0.01, 0.001],\n",
              " 'units': [32,\n",
              "  64,\n",
              "  96,\n",
              "  128,\n",
              "  160,\n",
              "  192,\n",
              "  224,\n",
              "  256,\n",
              "  288,\n",
              "  320,\n",
              "  352,\n",
              "  384,\n",
              "  416,\n",
              "  448,\n",
              "  480,\n",
              "  512]}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c8p4RjII0O7"
      },
      "source": [
        "### Build a Sklearn Compatible Model Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irzGZqpHI0O7"
      },
      "source": [
        "def build_model(units, learning_rate, activation):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a compile keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units, activation=activation))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGGbBttyI0O7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210a692f-ff1c-4cb3-afd9-ebe1b3db9b00"
      },
      "source": [
        "model = KerasClassifier(build_fn = build_model)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "-6xsKyfLI0O7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936a4c8f-1fe8-48c0-a4b7-9ba114bf57e1"
      },
      "source": [
        "# save start time \n",
        "start = time()\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=hyper_parameters, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "\"\"\"\n",
        "grid_result = grid.fit(X_train, y_train) UNCOMMENT IF NECESSARY\n",
        "\"\"\"\n",
        "# save end time \n",
        "end = time()\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.0444 - accuracy: 0.2329\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.1566 - accuracy: 0.1612\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.9192 - accuracy: 0.2888\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.1584 - accuracy: 0.1578\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.9109 - accuracy: 0.2777\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9427 - accuracy: 0.2739\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.1156 - accuracy: 0.2014\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0922 - accuracy: 0.1901\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8917 - accuracy: 0.3249\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8739 - accuracy: 0.3258\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.9871 - accuracy: 0.2743\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0043 - accuracy: 0.2454\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.8843 - accuracy: 0.3220\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9044 - accuracy: 0.2737\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9126 - accuracy: 0.3186\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8509 - accuracy: 0.3217\n",
            "1563/1563 [==============================] - 11s 2ms/step - loss: 1.9043 - accuracy: 0.3350\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.1689 - accuracy: 0.2567\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.8926 - accuracy: 0.3372\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8622 - accuracy: 0.3381\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1619 - accuracy: 0.2222\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1297 - accuracy: 0.1744\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9594 - accuracy: 0.3118\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9401 - accuracy: 0.2386\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9517 - accuracy: 0.3331\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.7917 - accuracy: 0.3399\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0570 - accuracy: 0.3023\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.8436 - accuracy: 0.1364\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.8701 - accuracy: 0.3611\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9340 - accuracy: 0.2263\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.1334 - accuracy: 0.2565\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.2181 - accuracy: 0.1586\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.0200 - accuracy: 0.2858\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9370 - accuracy: 0.2542\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.8649 - accuracy: 0.3840\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8445 - accuracy: 0.3778\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9258 - accuracy: 0.3482\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.1209 - accuracy: 0.3245\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0427 - accuracy: 0.2955\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.0522 - accuracy: 0.2458\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0554 - accuracy: 0.2975\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.0788 - accuracy: 0.2037\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9863 - accuracy: 0.3320\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.2083 - accuracy: 0.2959\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9463 - accuracy: 0.3317\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8484 - accuracy: 0.3138\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9446 - accuracy: 0.3276\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8529 - accuracy: 0.3114\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9753 - accuracy: 0.3234\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9250 - accuracy: 0.3067\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.1937 - accuracy: 0.2779\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0626 - accuracy: 0.2327\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0777 - accuracy: 0.3347\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9640 - accuracy: 0.3078\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0065 - accuracy: 0.3677\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.7845 - accuracy: 0.3410\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0351 - accuracy: 0.3712\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8854 - accuracy: 0.2740\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9963 - accuracy: 0.3417\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7786 - accuracy: 0.3326\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.2321 - accuracy: 0.2543\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0457 - accuracy: 0.2307\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0216 - accuracy: 0.3474\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8494 - accuracy: 0.3098\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0547 - accuracy: 0.3510\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0218 - accuracy: 0.3193\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9932 - accuracy: 0.4176\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9347 - accuracy: 0.3403\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0793 - accuracy: 0.3098\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0325 - accuracy: 0.2264\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8634 - accuracy: 0.4175\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7261 - accuracy: 0.3299\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1114 - accuracy: 0.3293\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9851 - accuracy: 0.2804\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0604 - accuracy: 0.3202\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9257 - accuracy: 0.2842\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1191 - accuracy: 0.3116\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0313 - accuracy: 0.2891\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.1885 - accuracy: 0.3513\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9513 - accuracy: 0.2520\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.3406 - accuracy: 0.2581\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9836 - accuracy: 0.2858\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.1713 - accuracy: 0.3184\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9834 - accuracy: 0.2553\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.1930 - accuracy: 0.3684\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8773 - accuracy: 0.3114\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0819 - accuracy: 0.3168\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8063 - accuracy: 0.3072\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.1725 - accuracy: 0.3693\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9592 - accuracy: 0.2684\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.0410 - accuracy: 0.3332\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7511 - accuracy: 0.3445\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.1611 - accuracy: 0.3347\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7985 - accuracy: 0.3129\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.4333 - accuracy: 0.2461\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1684 - accuracy: 0.1742\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8191 - accuracy: 0.7520\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7371 - accuracy: 0.7758\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8316 - accuracy: 0.7464\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7745 - accuracy: 0.7688\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7954 - accuracy: 0.7568\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6994 - accuracy: 0.7906\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7598 - accuracy: 0.7700\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6933 - accuracy: 0.7886\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7559 - accuracy: 0.7731\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6915 - accuracy: 0.7980\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7652 - accuracy: 0.7674\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6448 - accuracy: 0.8074\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7544 - accuracy: 0.7730\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7068 - accuracy: 0.7914\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7473 - accuracy: 0.7756\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6903 - accuracy: 0.7975\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7607 - accuracy: 0.7683\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6686 - accuracy: 0.8002\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7382 - accuracy: 0.7751\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6796 - accuracy: 0.7956\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7286 - accuracy: 0.7782\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7470 - accuracy: 0.7742\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7534 - accuracy: 0.7713\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6672 - accuracy: 0.7973\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7421 - accuracy: 0.7760\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6573 - accuracy: 0.7978\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7300 - accuracy: 0.7804\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6568 - accuracy: 0.8033\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7566 - accuracy: 0.7706\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6538 - accuracy: 0.8085\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7382 - accuracy: 0.7787\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6879 - accuracy: 0.7926\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7296 - accuracy: 0.7802\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6640 - accuracy: 0.8095\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7571 - accuracy: 0.7692\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6623 - accuracy: 0.8042\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7456 - accuracy: 0.7747\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6352 - accuracy: 0.8094\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7286 - accuracy: 0.7798\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6781 - accuracy: 0.8006\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7526 - accuracy: 0.7725\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6662 - accuracy: 0.8011\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7472 - accuracy: 0.7748\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7419 - accuracy: 0.7774\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7318 - accuracy: 0.7810\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6618 - accuracy: 0.8054\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7421 - accuracy: 0.7765\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6673 - accuracy: 0.8069\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7435 - accuracy: 0.7762\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6845 - accuracy: 0.8019\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7352 - accuracy: 0.7809\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6870 - accuracy: 0.8016\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7535 - accuracy: 0.7719\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6572 - accuracy: 0.8068\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7437 - accuracy: 0.7766\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6504 - accuracy: 0.8169\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7395 - accuracy: 0.7785\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6834 - accuracy: 0.8000\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7577 - accuracy: 0.7708\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6208 - accuracy: 0.8142\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7503 - accuracy: 0.7760\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6949 - accuracy: 0.7920\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7417 - accuracy: 0.7772\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6793 - accuracy: 0.7976\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7471 - accuracy: 0.7763\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6505 - accuracy: 0.8064\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7474 - accuracy: 0.7753\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6699 - accuracy: 0.7968\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7417 - accuracy: 0.7769\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6829 - accuracy: 0.7971\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7459 - accuracy: 0.7765\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6463 - accuracy: 0.8157\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7510 - accuracy: 0.7751\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6637 - accuracy: 0.8062\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7265 - accuracy: 0.7798\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6696 - accuracy: 0.8104\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7591 - accuracy: 0.7699\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6940 - accuracy: 0.7968\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7458 - accuracy: 0.7767\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6860 - accuracy: 0.8047\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7438 - accuracy: 0.7784\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6939 - accuracy: 0.7930\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7559 - accuracy: 0.7720\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6620 - accuracy: 0.8068\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7509 - accuracy: 0.7760\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6679 - accuracy: 0.8015\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7341 - accuracy: 0.7797\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7244 - accuracy: 0.7893\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.7522 - accuracy: 0.7737\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6621 - accuracy: 0.8010\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.7457 - accuracy: 0.7760\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6796 - accuracy: 0.8013\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7420 - accuracy: 0.7779\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6693 - accuracy: 0.8018\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7541 - accuracy: 0.7729\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6586 - accuracy: 0.8040\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8481 - accuracy: 0.7490\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7008 - accuracy: 0.7947\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8862 - accuracy: 0.7385\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7513 - accuracy: 0.7830\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8730 - accuracy: 0.7407\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7276 - accuracy: 0.7882\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7944 - accuracy: 0.7668\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6311 - accuracy: 0.8177\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7924 - accuracy: 0.7655\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6540 - accuracy: 0.8100\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8049 - accuracy: 0.7606\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6314 - accuracy: 0.8183\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7649 - accuracy: 0.7761\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6133 - accuracy: 0.8184\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7529 - accuracy: 0.7764\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6399 - accuracy: 0.8120\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7685 - accuracy: 0.7721\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6039 - accuracy: 0.8233\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7390 - accuracy: 0.7818\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5868 - accuracy: 0.8280\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7453 - accuracy: 0.7805\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6115 - accuracy: 0.8197\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7447 - accuracy: 0.7785\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5872 - accuracy: 0.8264\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7295 - accuracy: 0.7846\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5712 - accuracy: 0.8336\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7235 - accuracy: 0.7848\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5880 - accuracy: 0.8310\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7366 - accuracy: 0.7809\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5591 - accuracy: 0.8370\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7207 - accuracy: 0.7862\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5751 - accuracy: 0.8284\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7130 - accuracy: 0.7873\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5834 - accuracy: 0.8282\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7267 - accuracy: 0.7848\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5752 - accuracy: 0.8274\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7093 - accuracy: 0.7877\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5777 - accuracy: 0.8342\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7017 - accuracy: 0.7907\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5814 - accuracy: 0.8266\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7056 - accuracy: 0.7892\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5481 - accuracy: 0.8392\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6963 - accuracy: 0.7920\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5582 - accuracy: 0.8374\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6889 - accuracy: 0.7934\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5796 - accuracy: 0.8317\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7037 - accuracy: 0.7898\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5518 - accuracy: 0.8370\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6914 - accuracy: 0.7945\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5439 - accuracy: 0.8388\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6865 - accuracy: 0.7963\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5553 - accuracy: 0.8347\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6968 - accuracy: 0.7911\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5399 - accuracy: 0.8404\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6814 - accuracy: 0.7972\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5319 - accuracy: 0.8436\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6855 - accuracy: 0.7959\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5581 - accuracy: 0.8365\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6873 - accuracy: 0.7950\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5359 - accuracy: 0.8409\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6800 - accuracy: 0.7973\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5466 - accuracy: 0.8368\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6735 - accuracy: 0.7996\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5601 - accuracy: 0.8355\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6842 - accuracy: 0.7954\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5252 - accuracy: 0.8441\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6729 - accuracy: 0.7983\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5323 - accuracy: 0.8429\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6703 - accuracy: 0.7993\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5444 - accuracy: 0.8403\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6748 - accuracy: 0.7992\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5246 - accuracy: 0.8436\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6774 - accuracy: 0.7987\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5289 - accuracy: 0.8471\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6653 - accuracy: 0.7997\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5410 - accuracy: 0.8399\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6835 - accuracy: 0.7962\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5375 - accuracy: 0.8402\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6715 - accuracy: 0.8006\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5263 - accuracy: 0.8468\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6628 - accuracy: 0.8035\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5458 - accuracy: 0.8396\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6802 - accuracy: 0.7952\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5173 - accuracy: 0.8474\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6681 - accuracy: 0.8006\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5288 - accuracy: 0.8433\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6620 - accuracy: 0.8042\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5457 - accuracy: 0.8374\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6715 - accuracy: 0.8014\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5175 - accuracy: 0.8482\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6640 - accuracy: 0.8014\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5281 - accuracy: 0.8412\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6551 - accuracy: 0.8032\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5610 - accuracy: 0.8338\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6651 - accuracy: 0.8010\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5052 - accuracy: 0.8513\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1638 - accuracy: 0.6247\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0177 - accuracy: 0.6702\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1846 - accuracy: 0.6209\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1213 - accuracy: 0.6501\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1920 - accuracy: 0.6142\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1026 - accuracy: 0.6557\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1581 - accuracy: 0.6359\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1192 - accuracy: 0.6279\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2615 - accuracy: 0.5955\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1531 - accuracy: 0.6442\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1719 - accuracy: 0.6308\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0639 - accuracy: 0.6701\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2025 - accuracy: 0.6244\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.3558 - accuracy: 0.5752\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1468 - accuracy: 0.6364\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0256 - accuracy: 0.6823\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2146 - accuracy: 0.6190\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0226 - accuracy: 0.6811\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2781 - accuracy: 0.6180\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1950 - accuracy: 0.6602\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2462 - accuracy: 0.6195\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1298 - accuracy: 0.6428\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2249 - accuracy: 0.6238\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0549 - accuracy: 0.7110\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4021 - accuracy: 0.5971\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1052 - accuracy: 0.6598\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1879 - accuracy: 0.6341\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2611 - accuracy: 0.6319\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4477 - accuracy: 0.5827\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4503 - accuracy: 0.5822\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4652 - accuracy: 0.5853\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3006 - accuracy: 0.6583\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3678 - accuracy: 0.6122\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3788 - accuracy: 0.6318\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4217 - accuracy: 0.5819\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2031 - accuracy: 0.6297\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4790 - accuracy: 0.5937\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3643 - accuracy: 0.6314\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4787 - accuracy: 0.5937\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5042 - accuracy: 0.6188\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2381 - accuracy: 0.6181\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1981 - accuracy: 0.6353\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3426 - accuracy: 0.5951\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1471 - accuracy: 0.6696\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3350 - accuracy: 0.6088\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2370 - accuracy: 0.6217\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3830 - accuracy: 0.6102\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3078 - accuracy: 0.6574\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1804 - accuracy: 0.6316\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2253 - accuracy: 0.6321\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3286 - accuracy: 0.6140\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2069 - accuracy: 0.6619\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5947 - accuracy: 0.5856\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5954 - accuracy: 0.6066\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2290 - accuracy: 0.6233\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0875 - accuracy: 0.6433\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5643 - accuracy: 0.6001\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4420 - accuracy: 0.6377\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4085 - accuracy: 0.6015\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2458 - accuracy: 0.6480\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4139 - accuracy: 0.6097\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2975 - accuracy: 0.6109\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3929 - accuracy: 0.6163\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3055 - accuracy: 0.6264\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5617 - accuracy: 0.6048\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9040 - accuracy: 0.5727\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5653 - accuracy: 0.6011\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4995 - accuracy: 0.5994\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4089 - accuracy: 0.6148\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2648 - accuracy: 0.6698\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6931 - accuracy: 0.5819\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.8996 - accuracy: 0.5586\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0387 - accuracy: 0.5655\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.4798 - accuracy: 0.6551\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4855 - accuracy: 0.6131\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.5685 - accuracy: 0.6352\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5617 - accuracy: 0.5769\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.3902 - accuracy: 0.5836\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7532 - accuracy: 0.5931\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.5159 - accuracy: 0.6134\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5784 - accuracy: 0.5932\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.3715 - accuracy: 0.6437\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4763 - accuracy: 0.6114\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.2096 - accuracy: 0.6577\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4065 - accuracy: 0.6140\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.6924 - accuracy: 0.6099\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3750 - accuracy: 0.6217\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.1359 - accuracy: 0.6822\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4251 - accuracy: 0.6067\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.3542 - accuracy: 0.6530\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 2.2718 - accuracy: 0.5772\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.5880 - accuracy: 0.6803\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.6960 - accuracy: 0.5887\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.4095 - accuracy: 0.6527\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.8215 - accuracy: 0.5813\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 1.5234 - accuracy: 0.6385\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8068 - accuracy: 0.7565\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7034 - accuracy: 0.7854\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7795 - accuracy: 0.7611\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7144 - accuracy: 0.7780\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7978 - accuracy: 0.7585\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6830 - accuracy: 0.7944\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7337 - accuracy: 0.7762\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6379 - accuracy: 0.8079\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7238 - accuracy: 0.7791\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6477 - accuracy: 0.8065\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7435 - accuracy: 0.7707\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6376 - accuracy: 0.8075\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7063 - accuracy: 0.7857\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6113 - accuracy: 0.8167\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6987 - accuracy: 0.7860\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6439 - accuracy: 0.8056\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7215 - accuracy: 0.7786\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6236 - accuracy: 0.8114\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7089 - accuracy: 0.7829\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5802 - accuracy: 0.8264\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6838 - accuracy: 0.7913\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6147 - accuracy: 0.8141\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7032 - accuracy: 0.7849\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5865 - accuracy: 0.8265\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6949 - accuracy: 0.7875\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5974 - accuracy: 0.8201\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6902 - accuracy: 0.7865\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6155 - accuracy: 0.8138\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6961 - accuracy: 0.7870\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5869 - accuracy: 0.8242\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6963 - accuracy: 0.7852\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6208 - accuracy: 0.8125\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6850 - accuracy: 0.7885\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6420 - accuracy: 0.8027\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6958 - accuracy: 0.7853\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5948 - accuracy: 0.8222\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6856 - accuracy: 0.7892\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5966 - accuracy: 0.8189\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6806 - accuracy: 0.7922\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6083 - accuracy: 0.8133\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7043 - accuracy: 0.7826\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5904 - accuracy: 0.8203\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6866 - accuracy: 0.7889\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5785 - accuracy: 0.8228\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6805 - accuracy: 0.7901\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6146 - accuracy: 0.8144\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6971 - accuracy: 0.7852\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6337 - accuracy: 0.7992\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6895 - accuracy: 0.7898\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6068 - accuracy: 0.8164\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6770 - accuracy: 0.7921\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5926 - accuracy: 0.8202\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6885 - accuracy: 0.7874\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5821 - accuracy: 0.8268\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6855 - accuracy: 0.7905\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5875 - accuracy: 0.8221\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6847 - accuracy: 0.7902\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5974 - accuracy: 0.8187\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6883 - accuracy: 0.7892\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6034 - accuracy: 0.8129\n",
            " 167/1563 [==>...........................] - ETA: 6s - loss: 1.0364 - accuracy: 0.6841"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9xIXvLHI0O8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77eb04b3-204b-4508-fd80-e7029070f304"
      },
      "source": [
        "# total run time \n",
        "total_run_time_in_minutes = (end - start)/60\n",
        "total_run_time_in_minutes"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44.92918124993642"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDhmB1KCI0O8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c65b82-b3a8-4d1d-aa3d-23a2e9c4cd56"
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu', 'learning_rate': 0.001, 'units': 448}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfKXnHB5I0O8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c68eb7-2507-4927-f704-aadb650a70b8"
      },
      "source": [
        "# because all other optimization approaches are reporting test set score\n",
        "# let's calculate the test set score in this case \n",
        "best_model = grid_result.best_estimator_\n",
        "test_acc = best_model.score(X_test, y_test)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4966 - accuracy: 0.8529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFg1V-6JI0O8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77c6643-fc58-4319-d50c-29e8021c6dcd"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8529199957847595"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6aG0L0hI0O8"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the best performing hyperparameter combination and model score.\n",
        " \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9577db883482c6cded3836e5cfbf5a74",
          "grade": true,
          "grade_id": "cell-eb06d682d2790f6e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Kd07_qtdI0O8"
      },
      "source": [
        "{'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
        "\n",
        "loss: 0.4966 - accuracy: 0.8529"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkIWTM_RI0O8"
      },
      "source": [
        "_______\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches. \n",
        "\n",
        "Even if we found a way to pass the original test set into GridSearchCV, we could see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5hq4bEgI0O8"
      },
      "source": [
        "----\n",
        "\n",
        "# Stretch Goals\n",
        "\n",
        "- Feel free to run whatever gridsearch experiments on whatever models you like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9jYKevI0O8"
      },
      "source": [
        "# this is your open playground - be free to explore as you wish "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}